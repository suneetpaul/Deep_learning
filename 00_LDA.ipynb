{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXT1+M2JhlbWhY9iseany1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7D2kbGYJAR3i"
      },
      "outputs": [],
      "source": [
        "document = [\"Machine learning is amazing! \",\n",
        "           \"deep learning is a subset of machine learning.\",\n",
        "           \"natural language processing is a part of AI.\",\n",
        "           \"AI is transforming the world.\"]\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(max_df=0.95 , min_df=2 , stop_words='english')\n",
        "\n",
        "dtm = cv.fit_transform(document)   # conveert text to document - term matrix\n",
        "\n",
        "print(cv.get_feature_names_out())   # display remaining words\n",
        "print(dtm.toarray())    #show word counts is documnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGJFTMuNA45U",
        "outputId": "bbf07bc3-f970-44db-eb9e-dd629821dd05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ai' 'learning' 'machine']\n",
            "[[0 1 1]\n",
            " [0 2 1]\n",
            " [1 0 0]\n",
            " [1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document = [\"Machine learning is amazing! \",\n",
        "           \"deep learning is a subset of machine learning.\",\n",
        "           \"natural language processing is a part of AI.\",\n",
        "           \"AI is transforming the world.\"]\n",
        "\n",
        "cv = CountVectorizer(stop_words='english')\n",
        "\n",
        "dtm = cv.fit_transform(document)   # conveert text to document - term matrix\\\n",
        "feature_names = cv.get_feature_names_out()\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "for i in range(1 , 10):\n",
        "    random_word_id = random.randint(0 , len(feature_names)-1)\n",
        "    print(feature_names[random_word_id])\n"
      ],
      "metadata": {
        "id": "HhNu4AqGBi-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24d6e52c-4994-45d6-f54a-533c9bfb9c59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deep\n",
            "deep\n",
            "machine\n",
            "language\n",
            "subset\n",
            "processing\n",
            "natural\n",
            "ai\n",
            "processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "doc = [\n",
        "    \"I love machine learning and Data science.\",\n",
        "    \"Deep learning is a subset of Machine learning.\",\n",
        "    \"I enjoy playing football and watching sports.\",\n",
        "    \"Sports analytics is an interesting field.\",\n",
        "\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "dtm = vectorizer.fit_transform(doc)\n",
        "\n",
        "#train lda model\n",
        "lda = LatentDirichletAllocation(n_components=2 , random_state= 40)\n",
        "lda.fit(dtm)\n",
        "\n",
        "#get topic distribution for each doc\n",
        "topic_results = lda.transform(dtm)\n",
        "\n",
        "#identify the dominat topic for each doc\n",
        "dominant_topics = topic_results.argmax(axis=1)\n",
        "\n",
        "#print result\n",
        "\n",
        "for i , topic in enumerate(dominant_topics):\n",
        "  print(f\"doc{i+1} is mostly about topic{topic}\")"
      ],
      "metadata": {
        "id": "wt_CkeGWEYNq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "727a21ab-cfe8-4a02-82fb-954993e32e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doc1 is mostly about topic1\n",
            "doc2 is mostly about topic1\n",
            "doc3 is mostly about topic0\n",
            "doc4 is mostly about topic0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sj87BKz2GxX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train LDA  model lda.fit(dtm)\n",
        "\n",
        "LDA learn\n",
        "\n",
        "topic - word distrribution\n",
        "\n",
        "Document - topic distribution\n",
        "\n",
        "uses probabilistic inference internally\n",
        "\n",
        "get topic distribution per docment\n",
        "\n",
        "topic_results = lda.transform(dtm)\n",
        "\n",
        "return probsbilty of each topic in each dosument\n",
        "\n",
        "shape :\n",
        "\n",
        "number of document * number of topic\n",
        "\n",
        "examle:\n",
        "\n",
        "[0.90 , 0.10] --> topic 0, 10% topic 1"
      ],
      "metadata": {
        "id": "mCxCoTWwHQWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find domint tpic\n",
        "\n",
        "dominat _topic = topic_resukt .argmax(axis=1)\n",
        "\n",
        "argmax(axis =1 ) selects the topic with higest probabilty\n",
        "\n",
        "each docmunent is assigned its most dominant topic\n",
        "\n",
        "example :\n",
        "\n",
        "[0,0,1,1]"
      ],
      "metadata": {
        "id": "XR9asRRHIn9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "print filan result\n",
        "\n",
        "for i , topic in enumerate(dominant_topics):\n",
        "  print(f\"doc{i+1} is mostly about topic{topic}\")\n",
        "\n",
        "  lpops through doc\n",
        "\n"
      ],
      "metadata": {
        "id": "M-INfplBJGy3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XztFVu7IIlQo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}